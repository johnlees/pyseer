#!/usr/bin/env python
'''Python reimplementation of SEER for bacterial GWAS

Copyright 2017 Marco Galardini and John Lees'''

__version__ = '0.2.1'

import contextlib
from collections import namedtuple

Seer = namedtuple('Seer', ['kmer',
                           'af', 'prep', 'lrt_pvalue',
                           'kbeta', 'bse',
                           'intercept', 'betas',
                           'kstrains', 'nkstrains',
                           'notes',
                           'prefilter', 'filter'])

def get_options():
    import argparse

    description = 'SEER (doi: 10.1038/ncomms12797), reimplemented in python'
    parser = argparse.ArgumentParser(description=description)

    parser.add_argument('kmers',
                        help='Kmers file')
    parser.add_argument('phenotypes',
                        help='Phenotypes file')
    parser.add_argument('distances',
                        help='Strains distance square matrix')

    parser.add_argument('--continuous',
                        action='store_true',
                        default=False,
                        help='Force continuous phenotype [Default: binary auto-detect]')
    parser.add_argument('--print-samples',
                        action='store_true',
                        default=False,
                        help='Print sample lists [Default: hide samples]')
    parser.add_argument('--min-af',
                        type=float,
                        default=0.01,
                        help='Minimum AF [Default: 0.01]')
    parser.add_argument('--max-af',
                        type=float,
                        default=0.99,
                        help='Maximum AF [Default: 0.99]')
    parser.add_argument('--filter-pvalue',
                        type=float,
                        default=1,
                        help='Prefiltering t-test pvalue threshold [Default: 1]')
    parser.add_argument('--lrt-pvalue',
                        type=float,
                        default=1,
                        help='Likelihood ratio test pvalue threshold [Default: 1]')
    parser.add_argument('--max-dimensions',
                        type=int,
                        default=10,
                        help='Maximum number of dimensions to consider after MDS [Default: 10]')
    parser.add_argument('--covariates',
                        default=None,
                        help='User-defined covariates file (tab-delimited, no header, ' +
                             'first column contains sample names)')
    parser.add_argument('--use-covariates',
                        default=None,
                        nargs='*',
                        help='Covariates to use. Format is "2 3q 4" (q for quantitative)'
                             ' [Default: load covariates but don\'t use them]')
    parser.add_argument('--uncompressed',
                        action='store_true',
                        default=False,
                        help='Uncompressed kmers file [Default: gzipped]')
    parser.add_argument('--cpu',
                        type=int,
                        default=1,
                        help='Processes [Default: 1]')
    parser.add_argument('--save-m',
                        help='Prefix for saving matrix decomposition')
    scree_group = parser.add_mutually_exclusive_group()
    scree_group.add_argument('--load-m',
                        help='Load an existing matrix decomposition')
    scree_group.add_argument('--scree-plot',
                        action='store_true',
                        default=False,
                        help='Output MDS eigenvalues for a scree plot (exits without performing associations)')

    parser.add_argument('--version', action='version',
                        version='%(prog)s '+__version__)

    return parser.parse_args()


# thanks to Laurent LAPORTE on SO
@contextlib.contextmanager
def set_env(**environ):
    """
    Temporarily set the process environment variables.

    >>> with set_env(PLUGINS_DIR=u'test/plugins'):
    ...   "PLUGINS_DIR" in os.environ
    True

    >>> "PLUGINS_DIR" in os.environ
    False
    """
    old_environ = dict(os.environ)
    os.environ.update(environ)
    try:
        yield
    finally:
        os.environ.clear()
        os.environ.update(old_environ)


# thanks to Francis Song for this function
# source: http://www.nervouscomputer.com/hfs/cmdscale-in-python/
def cmdscale(D):
    """
    Classical multidimensional scaling (MDS)

    Parameters
    ----------
    D : (n, n) array
        Symmetric distance matrix.

    Returns
    -------
    Y : (n, p) array
        Configuration matrix. Each column represents a dimension. Only the
        p dimensions corresponding to positive eigenvalues of B are returned.
        Note that each dimension is only determined up to an overall sign,
        corresponding to a reflection.

    e : (n,) array
        Eigenvalues of B.
    """
    # Number of points
    n = len(D)

    # Centering matrix
    H = np.eye(n) - np.ones((n, n))/n

    # YY^T
    B = -H.dot(D**2).dot(H)/2

    # Diagonalize
    evals, evecs = np.linalg.eigh(B)

    # Sort by eigenvalue in descending order
    idx   = np.argsort(evals)[::-1]
    evals = evals[idx]
    evecs = evecs[:,idx]

    # Compute the coordinates using positive-eigenvalued components only
    w, = np.where(evals > 0)
    L  = np.diag(np.sqrt(evals[w]))
    V  = evecs[:,w]
    Y  = V.dot(L)

    return Y, evals[evals > 0]


def iter_kmers(p, m, cov, infile, all_strains,
               min_af, max_af,
               filter_pvalue, lrt_pvalue, null_fit, firth_null):
    for l in infile:
        if not options.uncompressed:
            l = l.decode()
        kmer, strains = l.split()[0], l.rstrip().split('|')[1].lstrip().split()

        d = {x.split(':')[0]: 1
             for x in strains}
        kstrains = sorted(set(d.keys()).intersection(all_strains))
        nkstrains = sorted(all_strains.difference(
                           all_strains.intersection(
                           {x.split(':')[0] for x in strains})))
        for x in nkstrains:
            d[x] = 0

        af = float(len(kstrains)) / len(all_strains)
        # filter by AF
        if af < min_af or af > max_af:
            # pass it to the actual tests to keep track
            yield (kmer, None, None, None, None, af,
                   None, None, None, None,
                   kstrains, nkstrains)
            continue

        v = p.values
        k = np.array([d[x] for x in p.index
                      if x in d])
        c = cov.values

        yield (kmer, v, k, m, c, af,
               filter_pvalue, lrt_pvalue, null_fit, firth_null,
               kstrains, nkstrains)


def binary(kmer, p, k, m, c, af,
           pret, lrtt, null_res, null_firth,
           kstrains, nkstrains):
    notes = set()

    # was this af-filtered?
    if p is None:
        notes.add('af-filter')
        return Seer(kmer, af, np.nan, np.nan,
                    np.nan, np.nan, np.nan, [],
                    kstrains, nkstrains,
                    notes, True, False)

    # pre-filtering
    t = np.concatenate((p.reshape(-1, 1), k.reshape(-1, 1)), axis=1).T
    table = [[t[0][(t[0] == 1) & (t[1] == 1)].shape[0],
              t[0][(t[0] == 1) & (t[1] == 0)].shape[0]],
             [t[0][(t[0] == 0) & (t[1] == 1)].shape[0],
              t[0][(t[0] == 0) & (t[1] == 0)].shape[0]]]

    # check for small values
    bad_chisq = 0
    bad_entries = 0
    for row in table:
        for entry in row:
            if entry <= 1:
                bad_chisq = True
                notes.add('bad-chisq')
            elif entry <= 5:
                bad_entries += 1
    if bad_entries > 1:
        bad_chisq = True
        notes.add('bad-chisq')

    prep = stats.chi2_contingency(table, correction=False)[1]
    if prep > pret or not np.isfinite(prep):
        notes.add('pre-filtering-failed')
        return Seer(kmer, af, prep, np.nan,
                    np.nan, np.nan, np.nan, [],
                    kstrains, nkstrains,
                    notes, True, False)

    # actual logistic regression
    if c.shape[0] == m.shape[0]:
        v = np.concatenate((np.ones(m.shape[0]).reshape(-1, 1),
                            k.reshape(-1, 1),
                            m,
                            c),
                           axis=1)
    else:
        # no covariates
        v = np.concatenate((np.ones(m.shape[0]).reshape(-1, 1),
                            k.reshape(-1, 1),
                            m),
                           axis=1)
    mod = smf.Logit(p, v)

    start_vec = np.zeros(v.shape[1])
    start_vec[0] = np.log(np.mean(p)/(1-np.mean(p)))

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        if not bad_chisq:
            try:
                res = mod.fit(start_params=start_vec, method='newton')

                if res.bse[1] > 3:
                    bad_chisq = True
                    notes.add('high-bse')
                else:
                    lrstat = -2*(null_res - res.llf)
                    lrt_pvalue = 1
                    if lrstat > 0: # non-convergence
                        lrt_pvalue = stats.chi2.sf(lrstat, 1)

                    intercept = res.params[0]
                    kbeta = res.params[1]
                    beta = res.params[2:]
                    bse = res.bse[1]
            except statsmodels.tools.sm_exceptions.PerfectSeparationError:
                bad_chisq = True
                notes.add('perfectly-separable-data')

        # Fit Firth regression with large SE, or nearly separable values
        if bad_chisq:
            firth_fit = fit_firth(mod, start_vec, kmer, v, p)
            if firth_fit is None: # Firth failure
                notes.add('firth-fail')
                return Seer(kmer, af, prep, np.nan,
                            np.nan, np.nan, np.nan, [],
                            kstrains, nkstrains,
                            notes, False, True)
            else:
                intercept, kbeta, beta, bse, fitll = firth_fit
                lrstat = -2*(null_firth - fitll)
                lrt_pvalue = 1
                if lrstat > 0: # non-convergence
                    lrt_pvalue = stats.chi2.sf(lrstat, 1)
    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        notes.add('matrix-inversion-error')
        return Seer(kmer, af, prep, np.nan,
                    np.nan, np.nan, np.nan, [],
                    kstrains, nkstrains,
                    notes, False, True)
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    if lrt_pvalue > lrtt or not np.isfinite(lrt_pvalue) or not np.isfinite(kbeta):
        notes.add('lrt-filtering-failed')
        return Seer(kmer, af, prep, lrt_pvalue,
                    kbeta, bse, intercept, beta,
                    kstrains, nkstrains,
                    notes, False, True)

    return Seer(kmer, af, prep, lrt_pvalue,
                kbeta, bse, intercept, beta,
                kstrains, nkstrains,
                notes, False, False)


def firth_likelihood(beta, logit):
    return -(logit.loglike(beta) + 0.5*np.log(np.linalg.det(-logit.hessian(beta))))


# Do firth regression
# Note information = -hessian, for some reason available but not implemented in statsmodels
def fit_firth(logit_model, start_vec, kmer_name,
              X, y, step_limit=1000, convergence_limit=0.0001):

    beta_iterations = []
    beta_iterations.append(start_vec)
    for i in range(0, step_limit):
        pi = logit_model.predict(beta_iterations[i])
        W = np.diagflat(np.multiply(pi, 1-pi))
        var_covar_mat = np.linalg.pinv(-logit_model.hessian(beta_iterations[i]))

        # build hat matrix
        rootW = np.sqrt(W)
        H = np.dot(np.transpose(X), np.transpose(rootW))
        H = np.matmul(var_covar_mat, H)
        H = np.matmul(np.dot(rootW, X), H)

        # penalised score
        U = np.matmul(np.transpose(X), p.values - pi + np.multiply(np.diagonal(H), 0.5 - pi))
        new_beta = beta_iterations[i] + np.matmul(var_covar_mat, U)

        # step halving
        j = 0
        while firth_likelihood(new_beta, logit_model) > firth_likelihood(beta_iterations[i], logit_model):
            new_beta = beta_iterations[i] + 0.5*(new_beta - beta_iterations[i])
            j = j + 1
            if (j > step_limit):
                return None

        beta_iterations.append(new_beta)
        if i > 0 and (np.linalg.norm(beta_iterations[i] - beta_iterations[i-1]) < convergence_limit):
            break

    return_fit = None
    if np.linalg.norm(beta_iterations[i] - beta_iterations[i-1]) >= convergence_limit:
        pass
    else:
        # Calculate stats
        fitll = -firth_likelihood(beta_iterations[-1], logit_model)
        intercept = beta_iterations[-1][0]
        kbeta = beta_iterations[-1][1]
        beta = beta_iterations[-1][2:].tolist()
        bse = math.sqrt(-logit_model.hessian(beta_iterations[-1])[1,1])

        return_fit = intercept, kbeta, beta, bse, fitll

    return return_fit


def continuous(kmer, p, k, m, c, af,
               pret, lrtt, null_res, null_firth,
               kstrains, nkstrains):
    notes = set()

    # was this af-filtered?
    if p is None:
        notes.add('af-filter')
        return Seer(kmer, af, np.nan, np.nan,
                    np.nan, np.nan, np.nan, [],
                    kstrains, nkstrains,
                    notes, True, False)

    # pre-filtering
    prep = stats.ttest_ind(p[k == 1],
                           p[k == 0],
                           equal_var=False)[1]
    if prep > pret or not np.isfinite(prep):
        notes.add('pre-filtering-failed')
        return Seer(kmer, af, prep, np.nan,
                    np.nan, np.nan, np.nan, [],
                    kstrains, nkstrains,
                    notes, True, False)

    # actual linear regression
    if c.shape[0] == m.shape[0]:
        v = np.concatenate((np.ones(m.shape[0]).reshape(-1, 1),
                            k.reshape(-1, 1),
                            m,
                            c),
                           axis=1)
    else:
        # no covariates
        v = np.concatenate((np.ones(m.shape[0]).reshape(-1, 1),
                            k.reshape(-1, 1),
                            m),
                           axis=1)
    mod = smf.OLS(p, v)

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        res = mod.fit()
        intercept = res.params[0]
        kbeta = res.params[1]
        beta = res.params[2:]
        bse = res.bse[1]
    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        # singular matrix error
        notes.add('matrix-inversion-error')
        return Seer(kmer, af, prep, np.nan,
                    np.nan, np.nan, np.nan, [],
                    kstrains, nkstrains,
                    notes, False, True)
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    lrt_pvalue = res.compare_lr_test(null_res)[1]
    if lrt_pvalue > lrtt or not np.isfinite(lrt_pvalue) or not np.isfinite(kbeta):
        notes.add('lrt-filtering-failed')
        return Seer(kmer, af, prep, lrt_pvalue,
                    kbeta, bse, intercept, beta,
                    kstrains, nkstrains,
                    notes, False, True)

    return Seer(kmer, af, prep, lrt_pvalue,
                kbeta, bse, intercept, beta,
                kstrains, nkstrains,
                notes, False, False)


# Fit the null model, regression without k-mer
def fit_null(p, m, cov, continuous, firth=False):
    if cov.shape[1] > 0:
        v = np.concatenate((p.values.reshape(-1, 1),
                            m,
                            cov.values),
                           axis=1)
    else:
        # no covariates
        v = np.concatenate((p.values.reshape(-1, 1),
                            m),
                           axis=1)

    df = pd.DataFrame(v,
                      columns=['phenotype'] +
                      ['PC%d'%x for x in range(1, m.shape[1]+1)] +
                      list(cov.columns))

    if cov.shape[1] > 0:
        formula = ('phenotype ~ ' +
                   ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]) + ' + ' +
                   ' + '.join(list(cov.columns)))
    else:
        formula = ('phenotype ~ ' +
                   ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]))

    if continuous:
        null_mod = smf.ols(formula=formula,
                           data=df)
    else:
        null_mod = smf.logit(formula=formula,
                             data=df)

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        if continuous:
            null_res = null_mod.fit()
        else:
            null_res = null_mod.fit(method='newton')
            if firth:
                null_res = -firth_likelihood(null_res.params, null_mod)
            else:
                null_res = null_res.llf
    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        sys.stderr.write('Matrix inversion error for null model\n')
        return None
    except statsmodels.tools.sm_exceptions.PerfectSeparationError:
        # singular matrix error
        sys.stderr.write('Perfetly separable data error for null model\n')
        return None
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    return null_res


def format_output(item, print_samples=False):
    out = '%s' % item.kmer
    out += '\t' + '\t'.join(['%.2E' % Decimal(x)
                             if np.isfinite(x)
                             else ''
                             for x in (item.af,
                                       item.prep,
                                       item.lrt_pvalue,
                                       item.kbeta,
                                       item.bse,
                                       item.intercept)])
    out += '\t' + '\t'.join(['%.2E' % x
                             if np.isfinite(x)
                             else ''
                             for x in item.betas])
    if print_samples:
        out += '\t' + '\t'.join((','.join(item.kstrains),
                                 ','.join(item.nkstrains)))
    out += '\t%s' % ','.join(item.notes)
    return out


if __name__ == "__main__":
    options = get_options()

    import sys

    # check some arguments here
    if options.max_dimensions < 1:
        sys.stderr.write('Minimum number of dimensions after MDS scaling is 1\n')
        sys.exit(1)
    if options.cpu > 1 and sys.version_info[0] < 3:
        sys.stderr.write('pyseer requires python version 3 or above ' +
                         'unless the number of threads is 1\n')
        sys.exit(1)
    import os

    # avoid numpy taking up more than one thread
    with set_env(MKL_NUM_THREADS='1',
                 NUMEXPR_NUM_THREADS='1',
                 OMP_NUM_THREADS='1'):
        import sys
        import gzip
        import warnings
        import itertools
        import math
        import statsmodels
        import numpy as np
        import pandas as pd
        from scipy import stats
        from decimal import Decimal
        from multiprocessing import Pool
        import statsmodels.formula.api as smf

        # silence warnings
        warnings.filterwarnings('ignore')
        #

        # reading phenotypes
        p = pd.Series([float(x.rstrip().split()[-1])
                       for x in open(options.phenotypes)],
                      index=[x.split()[0]
                             for x in open(options.phenotypes)])

        # Check whether any non 0/1 phenotypes
        if not options.continuous:
            if p.values[(p.values != 0) & (p.values != 1)].size > 0:
                options.continuous = True
                sys.stderr.write("Detected continuous phenotype\n")
            else:
                sys.stderr.write("Detected binary phenotype\n")

        # reading genome distances
        if options.load_m and os.path.isfile(options.load_m):
            m = pd.read_pickle(options.load_m)
            m = m.loc[p.index]
        else:
            m = pd.read_table(options.distances,
                          index_col=0)
            m = m.loc[p.index, p.index]
            # metric MDS scaling
            projection, evals = cmdscale(m)
            m = pd.DataFrame(projection,
                             index=m.index)
            for i in range(m.shape[1]):
                m[i] = m[i] / max(abs(m[i]))

            if options.save_m:
                m.to_pickle(options.save_m + ".pkl")

            if options.scree_plot:
                print('PC\teigenvalue')
                for i, e in enumerate(evals):
                    print('PC%d\t%.5f' % (i+1, e))
                sys.stderr.write('Exiting without performing associations\n')
                sys.exit(0)
        if options.max_dimensions > m.shape[1]:
            sys.stderr.write('Population MDS scaling restricted to '+
                             '%d dimensions instead of requested %d\n' % (m.shape[1],
                                                                          options.max_dimensions))
            options.max_dimensions = m.shape[1]
        m = m.values[:, :options.max_dimensions]

        all_strains = set(p.index)

        # read covariates
        if options.covariates is not None:
            c = pd.read_table(options.covariates,
                              header=None,
                              index_col=0)
            c.columns = ['covariate%d' % (x+2) for x in range(c.shape[1])]
            c = c.loc[p.index]
            # which covariates to use?
            if options.use_covariates is None:
                cov = pd.DataFrame([])
            else:
                cov = []
                for col in options.use_covariates:
                    cnum = int(col.rstrip('q'))
                    if cnum == 1 or cnum > c.shape[1] + 1:
                        sys.stderr.write('Covariates columns values should be > 1 and lower ' +
                                         'than total number of columns (%d)\n' % (c.shape[1] + 1))
                        sys.exit(1)
                    if col[-1] == 'q':
                        # quantitative
                        cov.append(c['covariate%d' % cnum])
                    else:
                        # categorical, dummy-encode it
                        categories = set(c['covariate%d' % cnum])
                        categories.pop()
                        for i, categ in enumerate(categories):
                            cov.append(pd.Series([1 if x == categ
                                                  else 0
                                                  for x in c['covariate%d' % cnum].values],
                                                 index=c.index,
                                                 name='covariate%d_%d' % (cnum, i)))
                cov = pd.concat(cov, axis=1)
        else:
            cov = pd.DataFrame([])

        header = ['kmer', 'af', 'filter-pvalue',
                  'lrt-pvalue', 'beta', 'beta-std-err',
                  'intercept'] + ['PC%d' % i for i in range(1, options.max_dimensions+1)]
        if options.covariates is not None:
            header = header + [x for x in cov.columns]
        if options.print_samples:
            header = header + ['k-samples', 'nk-samples']
        header += ['notes']
        print('\t'.join(header))

        if options.uncompressed:
            infile = open(options.kmers)
        else:
            infile = gzip.open(options.kmers, 'r')

        # multiprocessing setup
        if options.cpu > 1:
            pool = Pool(options.cpu)

        # calculate null regressions once
        null_fit = fit_null(p, m, cov, options.continuous)
        if not options.continuous:
            firth_null = fit_null(p, m, cov, options.continuous, True)
        else:
            firth_null = True

        if null_fit is None or firth_null is None:
            sys.stderr.write('Could not fit null model, exiting\n')
            sys.exit(1)

        # iterator over each kmer
        # implements maf filtering
        k_iter = iter_kmers(p, m, cov,
                            infile, all_strains,
                            options.min_af, options.max_af,
                            options.filter_pvalue,
                            options.lrt_pvalue, null_fit, firth_null)

        # keep track of the number of the total number of kmers and tests
        prefilter = 0
        tested = 0
        printed = 0

        # actual association test
        if options.cpu > 1:
            # multiprocessing proceeds 1000 kmers per core at a time
            if options.continuous:
                while True:
                    ret = pool.starmap(continuous,
                                       itertools.islice(k_iter,
                                                        options.cpu*1000))
                    if not ret:
                        break
                    for x in ret:
                        if x.prefilter:
                            prefilter += 1
                            continue
                        tested += 1
                        if x.filter:
                            continue
                        printed += 1
                        print(format_output(x,
                                            options.print_samples))
            else:
                while True:
                    ret = pool.starmap(binary,
                                       itertools.islice(k_iter,
                                                        options.cpu*1000))
                    if not ret:
                        break
                    for x in ret:
                        if x.prefilter:
                            prefilter += 1
                            continue
                        tested += 1
                        if x.filter:
                            continue
                        printed += 1
                        print(format_output(x,
                                            options.print_samples))
        else:
            for data in k_iter:
                if options.continuous:
                    ret = continuous(*data)
                else:
                    ret = binary(*data)
                if ret.prefilter:
                    prefilter += 1
                    continue
                tested += 1
                if ret.filter:
                    continue
                printed += 1
                print(format_output(ret,
                                    options.print_samples))

        sys.stderr.write('%d loaded kmers\n' % (prefilter + tested))
        sys.stderr.write('%d filtered kmers\n' % prefilter)
        sys.stderr.write('%d tested kmers\n' % tested)
        sys.stderr.write('%d printed kmers\n' % printed)
